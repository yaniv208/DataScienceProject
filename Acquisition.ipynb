{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2000c6d",
   "metadata": {},
   "source": [
    "### <u> Data Acquisition </u>\n",
    "\n",
    "\n",
    "### <u>Important note:</u> this file originally was written using PyCharm IDE, and was editted into a Jupyter notebook for ease of presentation. Only the concatination was written using Jupyter notebook.\n",
    "\n",
    "Let's start with imports:\n",
    "Pandas will help us handling DataFrames, and BeautifulSoup will help us with the parsing of the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc28a33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports used in the project\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import random\n",
    "import html\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fee2ce",
   "metadata": {},
   "source": [
    "----------\n",
    "Stating a List of user agents that will help us prevent response code 403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bd0d2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user agents were taken from this website:\n",
    "# https://www.whatismybrowser.com/guides/the-latest-user-agent/\n",
    "\n",
    "user_agent_list = [\n",
    "    # Chrome\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 5.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36'\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36'\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.93 Safari/537.36'\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36'\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.69 Safari/537.36'\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.54 Safari/537.36'\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36'\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36'\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.61 Safari/537.36'\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.54 Safari/537.36'\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36'\n",
    "\n",
    "    # Firefox\n",
    "    'Mozilla/4.0 (compatible; MSIE 9.0; Windows NT 6.1)',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0)',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (Windows NT 6.2; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.0; Trident/5.0)',\n",
    "    'Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; WOW64; Trident/6.0)',\n",
    "    'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; Trident/6.0)',\n",
    "    'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729)',\n",
    "    'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:59.0) Gecko/20100101 Firefox/59.0',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:95.0) Gecko/20100101 Firefox/95.0',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:94.0) Gecko/20100101 Firefox/94.0',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:93.0) Gecko/20100101 Firefox/93.0',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:92.0) Gecko/20100101 Firefox/92.0',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:91.0) Gecko/20100101 Firefox/91.0']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b36066",
   "metadata": {},
   "source": [
    "Let's define a method that changes the user agent in every iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ed69e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_agent():\n",
    "    return random.choice(user_agent_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e52129a",
   "metadata": {},
   "source": [
    "--------------\n",
    "## <u>THE MAIN PART OF THIS PHASE</u>\n",
    "As mentioned above, this script was written on PyCharm IDE, so I will show the implementation of the scraping script and the main() function, and explain it afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f748bda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_html(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser') # fetch HTML document of specific book\n",
    "\n",
    "    title = soup.find(itemprop='name')\n",
    "    if title is None:\n",
    "        title = \"\"\n",
    "    else:\n",
    "        title = title.text.strip()\n",
    "\n",
    "    author = soup.find(class_=\"authorName__container\")\n",
    "    if author is None:\n",
    "        author = \"\"\n",
    "    else:\n",
    "        author = author.text\n",
    "        author = author.replace(\", \", \"\")\n",
    "\n",
    "    genre = soup.find(class_='actionLinkLite bookPageGenreLink')\n",
    "    if genre is None:\n",
    "        genre = \"\"\n",
    "    else:\n",
    "        genre = genre.text\n",
    "\n",
    "    num_of_pages = soup.find(itemprop=\"numberOfPages\")\n",
    "    if num_of_pages is None:\n",
    "        num_of_pages = -1\n",
    "    else:\n",
    "        num_of_pages = num_of_pages.text\n",
    "        num_of_pages = num_of_pages.split(' ')[0]\n",
    "\n",
    "    description = soup.find(id='description')\n",
    "    if description is None:\n",
    "        description = \"\"\n",
    "    else:\n",
    "        description = description.text\n",
    "        description = description.replace('...more', '')\n",
    "\n",
    "    rating = soup.find(id='bookMeta')\n",
    "    if rating is None:\n",
    "        rating = -1\n",
    "    else:\n",
    "        rating = rating.text\n",
    "        rating = rating.strip()[:4]  # extracting the rating from the entire string\n",
    "\n",
    "    review_count = soup.find(itemprop=\"reviewCount\")\n",
    "    if review_count is None:\n",
    "        review_count = \"\"\n",
    "    else:\n",
    "        review_count = review_count.text\n",
    "\n",
    "    data = [title, author, genre, num_of_pages, description, review_count, rating]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860cf13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    columns = ['Title', 'Author', 'Genre', '# Of Pages', 'Description', 'Reviews Count', 'Rating']\n",
    "    books_data_frame = pd.DataFrame(columns=columns)\n",
    "    url_template = \"https://www.goodreads.com/book/show/{}\"\n",
    "\n",
    "    cache = []\n",
    "    counter = 0\n",
    "\n",
    "    # Iterating 10,000 SUCCESSFUL times\n",
    "    while books_data_frame.shape[0] < 10000:\n",
    "\n",
    "        # There are approximately 20 million books on site,\n",
    "        # we'll sample them by chunks.\n",
    "        random_book_id = random.randint(1, 20000000)\n",
    "        while random_book_id in cache:\n",
    "            # Checking we didn't sample an ID we already have.\n",
    "            random_book_id = random.randint(1, 20000000)\n",
    "\n",
    "        book_url = url_template.format(random_book_id)\n",
    "\n",
    "        referer = random.randint(1, 500)\n",
    "        referer = url_template.format(str(referer))\n",
    "        user_agent = get_random_agent()\n",
    "        headers = {'Accept-Encoding': 'gzip, deflate, br', 'Accept-Language': 'en-US,en;q=0.5', 'User-Agent': user_agent, 'Host': 'www.goodreads.com', 'referer': referer}\n",
    "\n",
    "        r = requests.get(book_url, headers=headers)\n",
    "        if r.status_code != 200:\n",
    "            print(\"Number of blocked requests: {}\".format(counter))\n",
    "            counter+= 1\n",
    "            print(\"Sleeping 6 seconds\")\n",
    "            time.sleep(6)\n",
    "            continue\n",
    "\n",
    "        # html.unescape is used in order to decode HTML files\n",
    "        data = extract_data_html(html.unescape(r.text))\n",
    "\n",
    "        # sleeping 3 seconds between each request so we can avoid response 403 (Forbidden)\n",
    "        time.sleep(3)\n",
    "\n",
    "        # creating a dictionary of each item to append to DataFrame\n",
    "        d = {col: data[i] for i, col in enumerate(columns)}\n",
    "\n",
    "        books_data_frame = books_data_frame.append(d, ignore_index=True)\n",
    "\n",
    "        # For debugging purposes\n",
    "        print(\"DataFrame size: \", books_data_frame.shape[0])\n",
    "\n",
    "\n",
    "    # Saving DataFrame to csv file.\n",
    "    books_data_frame.to_csv('dataframe6.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0346d45",
   "metadata": {},
   "source": [
    "## General explanation for main function and extract_data_html:\n",
    "\n",
    "This script generates a book ID (randomly) between specified samples, and reaches its HTML page in order to fetch the requested data using a method that I had to implement in order to scrape GoodReads.\n",
    "\n",
    "One of the problems I faced throughout the project was bypassing the 403s thrown by the server due to overloading their service with requests. In order to bypass their restriction, I had to to let the function sleep in intervals of 3 seconds each request (and 6 seconds whenever it got a 403 response code). Obviously this lead to a unreasonable amount of waiting time and in order to speed up the process I ran the script twice on three different computers each assigned different book IDs.\n",
    "\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001faecf",
   "metadata": {},
   "source": [
    " ### ** This part was written using Jupyter notebook **\n",
    "\n",
    "This code combines all DataFrames together into a one big DataFrame, and exports it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b3ce8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the csv files into a DataFrame\n",
    "dataframe1 = pd.read_csv(\"dataframe1.csv\")\n",
    "dataframe2 = pd.read_csv(\"dataframe2.csv\")\n",
    "dataframe3 = pd.read_csv(\"dataframe3.csv\")\n",
    "dataframe4 = pd.read_csv(\"dataframe4.csv\")\n",
    "dataframe5 = pd.read_csv(\"dataframe5.csv\")\n",
    "dataframe6 = pd.read_csv(\"dataframe6.csv\")\n",
    "\n",
    "# Concatinating all of the DataFrames\n",
    "combined_df = pd.concat([dataframe1, dataframe2, dataframe3, dataframe4, dataframe5, dataframe6])\n",
    "\n",
    "# Exporting the file\n",
    "combined_df.to_csv(\"combined_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8478e9",
   "metadata": {},
   "source": [
    "------------\n",
    "\n",
    "Data Acquisition phase completed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
